# Big Data Technologies  
## Final Project: Big Data Pipeline with Apache Spark &amp; MongoDB  

## Overview  
This project is a group-based assignment where you will design and implement a Big Data processing pipeline using Apache Spark and MongoDB. The goal is to apply the knowledge gained throughout the course to ingest, process, analyze, and store large-scale datasets effectively.  

## Team
- Janne Kauppila
- Antti Somero
- ...  

## Big data set  
t.b.d  

<<<<<<< HEAD
## Project Instructions
Set up your environment (Conda, Spark & MongoDB)
- MongoDB should be running on defaults
Download the sources and data set from: https://doi.org/10.25832/time_series/2020-10-06
Extract to a folder
Csv-file on Sourcedata folder is a dummy file, containing only a path to actual data file location (thanks to Github file size limit)
Download the data file and add it to the Sourcedata folder
Open the notebook with VSCode
Start the notebook, lean back and enjoy the ride!
---
=======
## Project Requirements  
Each team must: 

âœ” Choose a real-world dataset (e.g., Social Media, IoT, Financial, Healthcare, E-Commerce).  
âœ” Store the dataset in MongoDB using an optimized schema design.  
âœ” Process and analyze the data using Apache Spark (PySpark).  
âœ” Use Spark SQL for querying and comparing performance with MongoDB Aggregation Framework.  
âœ” Optimize performance using indexing, sharding, and partitioning in MongoDB.  
âœ” Present key insights with visualizations (Matplotlib, Seaborn, or another tool).  
âœ” Submit a final GitHub repository showcasing the full implementation with visible contributions from all team members.  

## Project Deliverables & Deadlines  

### ðŸ“Œ Assignment 1: Project Proposal  
Due Date: 6.4.2025  
Submission: 1-page summary (PDF or DOCX) via Moodle  
Contents:  

- Dataset choice & source

- Problem statement

- Project objectives

- Expected outcomes

### ðŸ“Œ Assignment 2: Final Report & Code Submission  
Due Date: 10.4.2025  
Submission:  

- Final Report (3-5 pages) (PDF or DOCX) including:

  - Introduction (Overview of the dataset and problem statement)

  - Methodology (Steps followed, tools used, schema design)

  - Implementation (Spark processing, MongoDB storage, query optimization)

  - Results & Insights (Key findings, visualizations)

  - Conclusions (Lessons learned, potential improvements)

- GitHub Repository Link

  - Codebase with a clear structure and documentation.

  - Contributions from all team members should be visible.

### Grading Criteria (Total: 10 Points)  
Criteria	Points
Dataset Selection & Preprocessing (Data choice, cleaning, storage)	2
Spark Implementation (Transformations, SQL Queries, Performance Optimization)	3
MongoDB Integration (Schema design, queries, indexing/sharding)	2
Results & Insights (Visualization, data storytelling)	1.5
Presentation & Report (Clarity, depth, technical explanation)	1.5

### Bonus Points (+1)  
âœ… Implement batch vs. stream processing using Spark Streaming.  
âœ… Compare query performance across different storage formats (CSV, Parquet, Avro).  
âœ… Build an interactive dashboard using Streamlit or Gradio.  

### Project Tools & Technologies  
- Apache Spark (PySpark) for large-scale data processing.  

- MongoDB (Local or Atlas) for NoSQL storage.  

- Google Colab / Jupyter Notebook for code execution.  

- Matplotlib, Seaborn for visualizations.  

- GitHub for version control and collaboration.  

### Submission Instructions  
- Upload your Project Proposal and Final Report to Moodle.  

- Provide a GitHub repository link with the full implementation.  

- Ensure all team members contribute and are credited in the report.  

For any questions or clarifications, feel free to reach out during office hours or via the course forum.    

ðŸš€ Good luck with your project!
>>>>>>> parent of 0850a6c (Final cut)
